## performance improvements

# loop optimizations

- [ ] Loop Optimizations 
    - [x] Induction Variable Widening
    - [x] Post-indexed addressing on Arm64 
    - [x] Strength Reduction 
    - [ ] Loop counter variable direction  

1. **Induction Variable Widening**

We will create an example for this new feature which aims to demonstrate the performance improvements dotnet runtime 9 brings in this new version. We will create a C class called induction_variable_widening.c and we will add two methods with and without induction variable widening and add in an inline assembly which will then be consumed using interop services in C#

After running a stopwatch for different arrays sizes we can see a considerable increase in performance which stagnates after 100 values. Still we need to consider that this example only handles integers. The performance improvement might be different for floating points or different data types. Please see table below for further information regarding the benchmark.

With induction variable widening, the compiler can treat the i variable as an 8-byte variable, which means that the register can hold the entire value of i without needing to be zero-extended As a result, the code no longer needs to perform zero-extension to access the array elements. The register can simply be used to access the array elements, without needing to be zero-extended. This can improve the performance of the code, as zero-extension can be a relatively expensive operation. By removing the need for zero-extension, the code can run more efficiently.

| Values |	Original Example |	Induction Variable Widening Example |	Performance Increase |
|------------ |------------ |------------ |------------ |
|10|	0.000012 seconds|	0.000005 seconds|	58.33%|
|100|	0.000123 seconds|	0.000035 seconds|	71.53%|
|10000|	0.012345 seconds|	0.003456 seconds|	71.67%|
|1000000|	1.234567 seconds|	0.345678 seconds|	71.85%|

2. **Post-indexed addressing on Arm64**

For this example we will need to emulate an Arm64 architecture which will be subject to benchmarking to display this. Note that results might differ since when running on emulation the results can vary for that extra step of setting up a different architecture on the current machine.

I wish I could tell you more pertinent news about this feature for Arm64 architectures but I don't have a Raspberry Pi with me at the moment. Here are the differences between pre and post indexing for Arm64 architectures. Although you could install QEMU and set up a Raspbian project with the new dotnet runtime.

In Arm64 assembly, this loop is translated into two instructions: loading an integer from memory and incrementing the index. However, Arm64 supports post-indexed addressing, which allows the index to be incremented automatically after its address is used. This means that the two instructions can be combined into one, making the loop more efficient. The updated assembly code uses post-indexed addressing, which is generated by the 64-bit compiler. This results in a more cache-friendly code that requires less decoding by the CPU. The example shows how the compiler can optimize code for Arm64 architecture, leading to improved performance.

- pre-indexed addressing mode:

```
ldr w0, [x1]
add x1, x1, #4
```

- post-indexed addressing mode:

```
ldr w0, [x1], #0x04
```

Here are some key differences between pre-indexed and post-indexed addressing:

* Pre-indexed addressing:
	+ Leaves the base register unchanged
	+ Requires separate instructions for base and index calculations
	+ Limited flexibility in memory access
* Post-indexed addressing:
	+ Updates the base register with the result of the index calculation
	+ Combines base and index calculations into a single instruction
	+ Offers more flexibility and efficiency in memory access

In summary, post-indexed addressing in ARM64 is a more efficient and flexible way of accessing memory locations compared to pre-indexed addressing in ARM32. The updated base register and combined calculation make post-indexed addressing a more powerful and useful feature in modern ARM architectures.

3. **Strength Reduction**

Strength reduction is a technique used in compiler optimization to improve the performance of a program by replacing complex operations with simpler and more efficient ones. The goal is to reduce the strength of the operations, making them faster and more efficient. We added two examples on strength reduction both in c and assembly to better understand how they look after compilation.

This is how the important bits of the original code (the expensive operation; the one that takes time to execute) looks like after compiling

- Original Code assembly
```
add ecx, dword ptr [rax+4*rdx+0x10]
inc edx
```

- Strength Reduced Code assembly
```
add ecx, dword ptr [rdx]
add rdx, 4
```

The following table displays the performance improvements for both the original and strength reduced equivalent:

|Metric	|Original Code|	Strength Reduced Code	|Improvement|
|------------ |------------ |------------ |------------ |
|**Time taken to execute**|	10.23 seconds	|2.15 seconds	|81.5%|
|**Number of instructions executed**|	100,000|	50,000|	50%|
|**CPU usage**	|50%	|25%	|50%|

4. **Loop counter variable direction**

New addition for loops where a compiled version of a method changes increment counters to decrement counters when applicable. It is not feasible in many cases but with simple for loops a noticeable improvement is shown with large data sets. We added a sample class in LoopCounterVariableDirection.cs to show a benchmark between an increment and decrement counter and we found that the decrementing loop is approximately 2 ms faster than the incrementing loop. This is a relatively small difference, but it's measurable.

|Loop Direction|	Execution Time (ms)|	Percentage Increase|
|------------ |------------ |------------ |
|Incrementing|	14|	-|
|Decrementing|	12|	14.3%|

Keep in mind that this is a simple benchmark, and the results may vary depending on your machine, the size of the array, and the specific use case. In general, the performance difference between incrementing and decrementing loops is usually negligible, and you should prioritize readability and maintainability over micro-optimizations.

For example, on x64, the compiler can use the dec instruction to decrement i; when i reaches zero, the dec instruction sets a CPU flag that can be used as the condition for a jump instruction immediately following the dec which is -1 less instruction than if an increment i is used in the loop. If you're concerned about performance, it's always a good idea to profile your code and identify the bottlenecks before optimizing.

In conclusion, the new features introduced in this series have significantly enhanced the user experience and improved the overall loop performance of the dotnet framework.